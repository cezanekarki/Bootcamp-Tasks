{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9481ce26-e4f9-4853-aa65-720bc4b8d17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DateType\n",
    "spark=SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bdca179e-3f5a-4137-acde-c31924a07480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/10_20220101.json</td><td>10_20220101.json</td><td>102</td><td>1707463030000</td></tr><tr><td>dbfs:/FileStore/tables/1_20220101.json</td><td>1_20220101.json</td><td>94</td><td>1707463030000</td></tr><tr><td>dbfs:/FileStore/tables/2_20220101.json</td><td>2_20220101.json</td><td>100</td><td>1707463031000</td></tr><tr><td>dbfs:/FileStore/tables/4_20220101.json</td><td>4_20220101.json</td><td>100</td><td>1707463031000</td></tr><tr><td>dbfs:/FileStore/tables/8_20220101.json</td><td>8_20220101.json</td><td>94</td><td>1707463031000</td></tr><tr><td>dbfs:/FileStore/tables/9_20220101.json</td><td>9_20220101.json</td><td>95</td><td>1707463030000</td></tr><tr><td>dbfs:/FileStore/tables/Address.xlsx</td><td>Address.xlsx</td><td>151315</td><td>1706079718000</td></tr><tr><td>dbfs:/FileStore/tables/Detail-1.csv</td><td>Detail-1.csv</td><td>208476</td><td>1706073951000</td></tr><tr><td>dbfs:/FileStore/tables/Detail.csv</td><td>Detail.csv</td><td>208476</td><td>1706073709000</td></tr><tr><td>dbfs:/FileStore/tables/Project1.xlsx</td><td>Project1.xlsx</td><td>422501</td><td>1706689323000</td></tr><tr><td>dbfs:/FileStore/tables/Project_1-1.xlsx</td><td>Project_1-1.xlsx</td><td>422501</td><td>1706173490000</td></tr><tr><td>dbfs:/FileStore/tables/Project_1-2.xlsx</td><td>Project_1-2.xlsx</td><td>422501</td><td>1706173950000</td></tr><tr><td>dbfs:/FileStore/tables/Project_1-3.xlsx</td><td>Project_1-3.xlsx</td><td>422501</td><td>1706174372000</td></tr><tr><td>dbfs:/FileStore/tables/Project_1-4.xlsx</td><td>Project_1-4.xlsx</td><td>422501</td><td>1706689008000</td></tr><tr><td>dbfs:/FileStore/tables/Project_1.xlsx</td><td>Project_1.xlsx</td><td>422501</td><td>1706173408000</td></tr><tr><td>dbfs:/FileStore/tables/batch.jsonl</td><td>batch.jsonl</td><td>671</td><td>1707463030000</td></tr><tr><td>dbfs:/FileStore/tables/contactinfo.txt</td><td>contactinfo.txt</td><td>49969</td><td>1706075893000</td></tr><tr><td>dbfs:/FileStore/tables/data/</td><td>data/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/final/</td><td>final/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/header-1.json</td><td>header-1.json</td><td>127562</td><td>1706159837000</td></tr><tr><td>dbfs:/FileStore/tables/header-2.json</td><td>header-2.json</td><td>127562</td><td>1706174817000</td></tr><tr><td>dbfs:/FileStore/tables/header.json</td><td>header.json</td><td>115560</td><td>1706076258000</td></tr><tr><td>dbfs:/FileStore/tables/header1.json</td><td>header1.json</td><td>429</td><td>1706077246000</td></tr><tr><td>dbfs:/FileStore/tables/project_1.ipynb</td><td>project_1.ipynb</td><td>9459630</td><td>1706689170000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/10_20220101.json",
         "10_20220101.json",
         102,
         1707463030000
        ],
        [
         "dbfs:/FileStore/tables/1_20220101.json",
         "1_20220101.json",
         94,
         1707463030000
        ],
        [
         "dbfs:/FileStore/tables/2_20220101.json",
         "2_20220101.json",
         100,
         1707463031000
        ],
        [
         "dbfs:/FileStore/tables/4_20220101.json",
         "4_20220101.json",
         100,
         1707463031000
        ],
        [
         "dbfs:/FileStore/tables/8_20220101.json",
         "8_20220101.json",
         94,
         1707463031000
        ],
        [
         "dbfs:/FileStore/tables/9_20220101.json",
         "9_20220101.json",
         95,
         1707463030000
        ],
        [
         "dbfs:/FileStore/tables/Address.xlsx",
         "Address.xlsx",
         151315,
         1706079718000
        ],
        [
         "dbfs:/FileStore/tables/Detail-1.csv",
         "Detail-1.csv",
         208476,
         1706073951000
        ],
        [
         "dbfs:/FileStore/tables/Detail.csv",
         "Detail.csv",
         208476,
         1706073709000
        ],
        [
         "dbfs:/FileStore/tables/Project1.xlsx",
         "Project1.xlsx",
         422501,
         1706689323000
        ],
        [
         "dbfs:/FileStore/tables/Project_1-1.xlsx",
         "Project_1-1.xlsx",
         422501,
         1706173490000
        ],
        [
         "dbfs:/FileStore/tables/Project_1-2.xlsx",
         "Project_1-2.xlsx",
         422501,
         1706173950000
        ],
        [
         "dbfs:/FileStore/tables/Project_1-3.xlsx",
         "Project_1-3.xlsx",
         422501,
         1706174372000
        ],
        [
         "dbfs:/FileStore/tables/Project_1-4.xlsx",
         "Project_1-4.xlsx",
         422501,
         1706689008000
        ],
        [
         "dbfs:/FileStore/tables/Project_1.xlsx",
         "Project_1.xlsx",
         422501,
         1706173408000
        ],
        [
         "dbfs:/FileStore/tables/batch.jsonl",
         "batch.jsonl",
         671,
         1707463030000
        ],
        [
         "dbfs:/FileStore/tables/contactinfo.txt",
         "contactinfo.txt",
         49969,
         1706075893000
        ],
        [
         "dbfs:/FileStore/tables/data/",
         "data/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/final/",
         "final/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/header-1.json",
         "header-1.json",
         127562,
         1706159837000
        ],
        [
         "dbfs:/FileStore/tables/header-2.json",
         "header-2.json",
         127562,
         1706174817000
        ],
        [
         "dbfs:/FileStore/tables/header.json",
         "header.json",
         115560,
         1706076258000
        ],
        [
         "dbfs:/FileStore/tables/header1.json",
         "header1.json",
         429,
         1706077246000
        ],
        [
         "dbfs:/FileStore/tables/project_1.ipynb",
         "project_1.ipynb",
         9459630,
         1706689170000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "caa6696a-176f-476a-8676-7afc3dde9777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-683965652416700>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_csv\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_csv' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-683965652416700>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_csv\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'df_csv' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_csv' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a644b0a2-9873-4df9-9763-a2098915ea29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"id\",IntegerType()),\n",
    "    StructField(\"name\",StringType()),\n",
    "    StructField(\"dob\",DateType()),\n",
    "    StructField(\"age\",IntegerType()),\n",
    "    StructField(\"salary\",IntegerType()),\n",
    "    StructField(\"department\",StringType()),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5a1f9b-88e6-48e4-9670-3be0a6197409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv=spark.read.format(\"csv\").schema(schema).option(\"header\",True).load(\"dbfs:///FileStore/tables/data/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e32613-f6f8-4a28-b7c1-52489c16e538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d01fca0-11b9-40fe-91f5-666c9c344f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").load(\"dbfs:///FileStore/tables/data/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8b5338-0be4-49c1-a010-24e5e441e6c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- department: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983e98bf-e926-42f9-b8e6-a18e3b2ad42f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8d1e93a5-1034-4b5f-86a9-631d500b098f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "696e3f99-3a27-4724-9deb-c3464f9629a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# db_json=df_json.orderBy(\"id\")\n",
    "# db_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9485b859-5361-4bab-8433-a4b520c64296",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'dob', 'age', 'salary', 'department'] ['age', 'department', 'dob', 'id', 'name', 'salary']\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.columns,df_json.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ec327a-4e86-4766-a9e3-e30c332a659b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=df_json.select(df_csv.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cce3ca-7189-4dd2-a10a-7303e2230413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'dob', 'age', 'salary', 'department'] ['id', 'name', 'dob', 'age', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.columns,df_json.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2623bd8-5b7f-4d06-b909-087c2dab195d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n+---+------+----------+----+------+----------+\nonly showing top 20 rows\n\nNone\n"
     ]
    }
   ],
   "source": [
    "df=df_json.union(df_csv)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eebb622-7504-4517-9e8c-10db84c84a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dob</th><th>age</th><th>salary</th><th>department</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>60000</td><td>HR</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>IT</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>65000</td><td>Finance</td></tr><tr><td>5</td><td>David</td><td>1981-12-18</td><td>41</td><td>90000</td><td>HR</td></tr><tr><td>6</td><td>Susan</td><td>1989-07-05</td><td>33</td><td>75000</td><td>Finance</td></tr><tr><td>7</td><td>Mike</td><td>1976-03-15</td><td>46</td><td>95000</td><td>IT</td></tr><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>60000</td><td>HR</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>IT</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>65000</td><td>Finance</td></tr><tr><td>5</td><td>David</td><td>1981-12-18</td><td>41</td><td>90000</td><td>HR</td></tr><tr><td>6</td><td>Susan</td><td>1989-07-05</td><td>33</td><td>75000</td><td>Finance</td></tr><tr><td>7</td><td>Mike</td><td>1976-03-15</td><td>46</td><td>95000</td><td>IT</td></tr><tr><td>10</td><td>Sophie</td><td>1992-06-30</td><td>30</td><td>62000</td><td>Finance</td></tr><tr><td>10</td><td>Sophie</td><td>1992-06-30</td><td>30</td><td>62000</td><td>Finance</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>90000</td><td>Finance</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>90000</td><td>Finance</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>70000</td><td>Finance</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>70000</td><td>Finance</td></tr><tr><td>9</td><td>James</td><td>1983-10-14</td><td>39</td><td>87000</td><td>IT</td></tr><tr><td>9</td><td>James</td><td>1983-10-14</td><td>39</td><td>87000</td><td>IT</td></tr><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>8</td><td>Lisa</td><td>1995-08-20</td><td>27</td><td>58000</td><td>HR</td></tr><tr><td>8</td><td>Lisa</td><td>1995-08-20</td><td>27</td><td>58000</td><td>HR</td></tr><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>60000</td><td>HR</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>IT</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>65000</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         60000,
         "HR"
        ],
        [
         3,
         "Bob",
         null,
         null,
         80000,
         "IT"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         65000,
         "Finance"
        ],
        [
         5,
         "David",
         "1981-12-18",
         41,
         90000,
         "HR"
        ],
        [
         6,
         "Susan",
         "1989-07-05",
         33,
         75000,
         "Finance"
        ],
        [
         7,
         "Mike",
         "1976-03-15",
         46,
         95000,
         "IT"
        ],
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         60000,
         "HR"
        ],
        [
         3,
         "Bob",
         null,
         null,
         80000,
         "IT"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         65000,
         "Finance"
        ],
        [
         5,
         "David",
         "1981-12-18",
         41,
         90000,
         "HR"
        ],
        [
         6,
         "Susan",
         "1989-07-05",
         33,
         75000,
         "Finance"
        ],
        [
         7,
         "Mike",
         "1976-03-15",
         46,
         95000,
         "IT"
        ],
        [
         10,
         "Sophie",
         "1992-06-30",
         30,
         62000,
         "Finance"
        ],
        [
         10,
         "Sophie",
         "1992-06-30",
         30,
         62000,
         "Finance"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         90000,
         "Finance"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         90000,
         "Finance"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         70000,
         "Finance"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         70000,
         "Finance"
        ],
        [
         9,
         "James",
         "1983-10-14",
         39,
         87000,
         "IT"
        ],
        [
         9,
         "James",
         "1983-10-14",
         39,
         87000,
         "IT"
        ],
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         8,
         "Lisa",
         "1995-08-20",
         27,
         58000,
         "HR"
        ],
        [
         8,
         "Lisa",
         "1995-08-20",
         27,
         58000,
         "HR"
        ],
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         60000,
         "HR"
        ],
        [
         3,
         "Bob",
         null,
         null,
         80000,
         "IT"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         65000,
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8903ed-481c-4970-9081-b9c2b9be8195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5803f299-1068-4d2d-9657-a82bc0488e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dob</th><th>age</th><th>salary</th><th>department</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>65000</td><td>Finance</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>60000</td><td>HR</td></tr><tr><td>6</td><td>Susan</td><td>1989-07-05</td><td>33</td><td>75000</td><td>Finance</td></tr><tr><td>7</td><td>Mike</td><td>1976-03-15</td><td>46</td><td>95000</td><td>IT</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>IT</td></tr><tr><td>5</td><td>David</td><td>1981-12-18</td><td>41</td><td>90000</td><td>HR</td></tr><tr><td>10</td><td>Sophie</td><td>1992-06-30</td><td>30</td><td>62000</td><td>Finance</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>90000</td><td>Finance</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>70000</td><td>Finance</td></tr><tr><td>9</td><td>James</td><td>1983-10-14</td><td>39</td><td>87000</td><td>IT</td></tr><tr><td>8</td><td>Lisa</td><td>1995-08-20</td><td>27</td><td>58000</td><td>HR</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         65000,
         "Finance"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         60000,
         "HR"
        ],
        [
         6,
         "Susan",
         "1989-07-05",
         33,
         75000,
         "Finance"
        ],
        [
         7,
         "Mike",
         "1976-03-15",
         46,
         95000,
         "IT"
        ],
        [
         3,
         "Bob",
         null,
         null,
         80000,
         "IT"
        ],
        [
         5,
         "David",
         "1981-12-18",
         41,
         90000,
         "HR"
        ],
        [
         10,
         "Sophie",
         "1992-06-30",
         30,
         62000,
         "Finance"
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         90000,
         "Finance"
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         70000,
         "Finance"
        ],
        [
         9,
         "James",
         "1983-10-14",
         39,
         87000,
         "IT"
        ],
        [
         8,
         "Lisa",
         "1995-08-20",
         27,
         58000,
         "HR"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a431dc-d0c9-4cbd-83fd-b4dbcdaab488",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 65000|  28|\n| 60000|  25|\n| 75000|  33|\n| 95000|  46|\n| 80000|null|\n| 90000|  41|\n| 62000|  30|\n| 90000|  25|\n| 70000|  28|\n| 87000|  39|\n| 58000|  27|\n+------+----+\n\n+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 65000|  28|\n| 60000|  25|\n| 75000|  33|\n| 95000|  46|\n| 80000|null|\n| 90000|  41|\n| 62000|  30|\n| 90000|  25|\n| 70000|  28|\n| 87000|  39|\n| 58000|  27|\n+------+----+\n\n+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 65000|  28|\n| 60000|  25|\n| 75000|  33|\n| 95000|  46|\n| 80000|null|\n| 90000|  41|\n| 62000|  30|\n| 90000|  25|\n| 70000|  28|\n| 87000|  39|\n| 58000|  27|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"salary\",\"age\").show()\n",
    "df.select(\"salary\",\"age\").show()\n",
    "\n",
    "df.select(df.salary,df.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee2cec82-4878-4771-998e-e9b2a5ef337a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 65000|  28|\n| 60000|  25|\n| 75000|  33|\n| 95000|  46|\n| 80000|null|\n| 90000|  41|\n| 62000|  30|\n| 90000|  25|\n| 70000|  28|\n| 87000|  39|\n| 58000|  27|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"salary\"),F.col(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec648a4-d765-4283-b95a-2d5bd6e10949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------+---------------------------------------+\n| salary|(year(current_timestamp()) - year(dob))|(year(current_timestamp()) - year(dob))|\n+-------+---------------------------------------+---------------------------------------+\n|73500.0|                                     32|                                     32|\n|68250.0|                                     30|                                     30|\n|63000.0|                                     27|                                     27|\n|78750.0|                                     35|                                     35|\n|99750.0|                                     48|                                     48|\n|84000.0|                                   null|                                   null|\n|94500.0|                                     43|                                     43|\n|65100.0|                                     32|                                     32|\n|94500.0|                                     27|                                     27|\n|73500.0|                                     30|                                     30|\n|91350.0|                                     41|                                     41|\n|60900.0|                                     29|                                     29|\n+-------+---------------------------------------+---------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    (df.salary+.05*df.salary).alias(\"salary\"),\n",
    "    F.year(F.current_timestamp())-F.year(\"dob\"),\n",
    "    F.year(F.current_timestamp())-F.year(F.col(\"dob\"))\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a749862b-317f-480c-a77b-5770a1cd23aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dob</th><th>age</th><th>salary</th><th>department</th><th>(salary + (salary * 0.05))</th><th>(salary + (salary * 0.05))</th><th>salary_raise</th><th>age</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>1992-05-12</td><td>30</td><td>70000</td><td>IT</td><td>73500.0</td><td>73500.0</td><td>73500.00</td><td>32</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>65000</td><td>Finance</td><td>68250.0</td><td>68250.0</td><td>68250.00</td><td>30</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>60000</td><td>HR</td><td>63000.0</td><td>63000.0</td><td>63000.00</td><td>27</td></tr><tr><td>6</td><td>Susan</td><td>1989-07-05</td><td>33</td><td>75000</td><td>Finance</td><td>78750.0</td><td>78750.0</td><td>78750.00</td><td>35</td></tr><tr><td>7</td><td>Mike</td><td>1976-03-15</td><td>46</td><td>95000</td><td>IT</td><td>99750.0</td><td>99750.0</td><td>99750.00</td><td>48</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>IT</td><td>84000.0</td><td>84000.0</td><td>84000.00</td><td>null</td></tr><tr><td>5</td><td>David</td><td>1981-12-18</td><td>41</td><td>90000</td><td>HR</td><td>94500.0</td><td>94500.0</td><td>94500.00</td><td>43</td></tr><tr><td>10</td><td>Sophie</td><td>1992-06-30</td><td>30</td><td>62000</td><td>Finance</td><td>65100.0</td><td>65100.0</td><td>65100.00</td><td>32</td></tr><tr><td>2</td><td>Alice</td><td>1997-02-28</td><td>25</td><td>90000</td><td>Finance</td><td>94500.0</td><td>94500.0</td><td>94500.00</td><td>27</td></tr><tr><td>4</td><td>Emily</td><td>1994-11-22</td><td>28</td><td>70000</td><td>Finance</td><td>73500.0</td><td>73500.0</td><td>73500.00</td><td>30</td></tr><tr><td>9</td><td>James</td><td>1983-10-14</td><td>39</td><td>87000</td><td>IT</td><td>91350.0</td><td>91350.0</td><td>91350.00</td><td>41</td></tr><tr><td>8</td><td>Lisa</td><td>1995-08-20</td><td>27</td><td>58000</td><td>HR</td><td>60900.0</td><td>60900.0</td><td>60900.00</td><td>29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "1992-05-12",
         30,
         70000,
         "IT",
         73500.0,
         73500.0,
         "73500.00",
         32
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         65000,
         "Finance",
         68250.0,
         68250.0,
         "68250.00",
         30
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         60000,
         "HR",
         63000.0,
         63000.0,
         "63000.00",
         27
        ],
        [
         6,
         "Susan",
         "1989-07-05",
         33,
         75000,
         "Finance",
         78750.0,
         78750.0,
         "78750.00",
         35
        ],
        [
         7,
         "Mike",
         "1976-03-15",
         46,
         95000,
         "IT",
         99750.0,
         99750.0,
         "99750.00",
         48
        ],
        [
         3,
         "Bob",
         null,
         null,
         80000,
         "IT",
         84000.0,
         84000.0,
         "84000.00",
         null
        ],
        [
         5,
         "David",
         "1981-12-18",
         41,
         90000,
         "HR",
         94500.0,
         94500.0,
         "94500.00",
         43
        ],
        [
         10,
         "Sophie",
         "1992-06-30",
         30,
         62000,
         "Finance",
         65100.0,
         65100.0,
         "65100.00",
         32
        ],
        [
         2,
         "Alice",
         "1997-02-28",
         25,
         90000,
         "Finance",
         94500.0,
         94500.0,
         "94500.00",
         27
        ],
        [
         4,
         "Emily",
         "1994-11-22",
         28,
         70000,
         "Finance",
         73500.0,
         73500.0,
         "73500.00",
         30
        ],
        [
         9,
         "James",
         "1983-10-14",
         39,
         87000,
         "IT",
         91350.0,
         91350.0,
         "91350.00",
         41
        ],
        [
         8,
         "Lisa",
         "1995-08-20",
         27,
         58000,
         "HR",
         60900.0,
         60900.0,
         "60900.00",
         29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "(salary + (salary * 0.05))",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "(salary + (salary * 0.05))",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "salary_raise",
         "type": "\"decimal(24,2)\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col(\"*\"),\n",
    "    df.salary+.05*df.salary,\n",
    "    (df.salary+.05*df.salary),\n",
    "    F.expr('salary+.05* salary').alias(\"salary_raise\"),\n",
    "    (F.year(F.current_timestamp())-F.year(F.col(\"dob\"))).alias(\"age\")\n",
    "    ).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79da2eb5-7521-4248-b914-b5ff8ec61a25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn(\n",
    "    \"salary_raise\",\n",
    "    (F.col(\"salary\")+.05*F.col(\"salary\"))\n",
    ").withColumn(\n",
    "    \"salary_raise_10_perc\",\n",
    "    (F.col(\"salary\")+.01*F.col(\"salary\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d425174-00c9-400a-978d-f7fb8a9cce10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumns(\n",
    "    {\n",
    "    \"salary_raise\":\n",
    "    (F.col(\"salary\")+.05*F.col(\"salary\")),\n",
    "    \"age\":(F.year(F.current_timestamp())-F.year(F.col(\"dob\")))\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e7277b-9f84-4e84-a9a5-3fbeb1fcabcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+------+----------+------------+--------------------+\n| id| name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|\n+---+-----+----------+----+------+----------+------------+--------------------+\n|  6|Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|\n|  7| Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|\n|  3|  Bob|      null|null| 80000|        IT|     84000.0|             80800.0|\n|  5|David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|\n|  2|Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|\n|  9|James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|\n+---+-----+----------+----+------+----------+------------+--------------------+\n\n+---+-----+----------+----+------+----------+------------+--------------------+\n| id| name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|\n+---+-----+----------+----+------+----------+------------+--------------------+\n|  6|Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|\n|  7| Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|\n|  3|  Bob|      null|null| 80000|        IT|     84000.0|             80800.0|\n|  5|David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|\n|  2|Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|\n|  9|James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|\n+---+-----+----------+----+------+----------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "    F.col(\"salary_raise\")>=75000\n",
    ").show()\n",
    "\n",
    "df.where(\n",
    "    F.col(\"salary_raise\")>=75000\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10906449-e31c-45f8-8d37-6ce4402cce90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df=df.withColumn(\n",
    "#     \"age_group\",\n",
    "#     F.when(\n",
    "#         F.col(\"age\")<=20,\n",
    "#         \"Upto 20\"\n",
    "#     ).when(\n",
    "#      ((F.col(\"age\")>20)&\n",
    "#         F.col(\"age\")<=30),\n",
    "#         \"21 to 30\"\n",
    "#     ).when(\n",
    "#      ((F.col(\"age\")>30)&\n",
    "#         F.col(\"age\")<=40),\n",
    "#         \"30 to 40\"\n",
    "#     ).otherwise(\n",
    "#        F.lit(\"More than 40\")\n",
    "#     )\n",
    "# )\n",
    "df = df.withColumn(\n",
    "    \"age_group\",\n",
    "    F.when(F.col(\"age\") <= 20, \"Upto 20\")\n",
    "    .when((F.col(\"age\") > 20) & (F.col(\"age\") <= 30), \"21 to 30\")\n",
    "    .when((F.col(\"age\") > 30) & (F.col(\"age\") <= 40), \"31 to 40\")\n",
    "    .otherwise(\"More than 40\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5745c318-a8b8-486f-be31-3384b72863c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37340396-bcb8-4a35-891f-8615ab81b284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0a9fd9-0675-4509-a661-17e1a048062c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+----------------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|min_age_by_group|\n+---+------+----------+----+------+----------+------------+--------------------+------------+----------------+\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|              27|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|              27|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|              27|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|              27|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|              27|\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|              32|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|              32|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|              32|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|              41|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|              41|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|              41|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|              41|\n+---+------+----------+----+------+----------+------------+--------------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window=Window.partitionBy(\"age_group\")\n",
    "\n",
    "df.withColumn(\n",
    "    \"min_age_by_group\",\n",
    "    F.min(\"age\").over(window)\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70468d7-0528-4c5f-9a27-57ffa5f7e608",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+-----------------+------------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|       avg_salary|is_above_avg|\n+---+------+----------+----+------+----------+------------+--------------------+------------+-----------------+------------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|75166.66666666667|       false|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|75166.66666666667|       false|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|75166.66666666667|       false|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|75166.66666666667|       false|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|75166.66666666667|        true|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|75166.66666666667|        true|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|75166.66666666667|        true|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|75166.66666666667|       false|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|75166.66666666667|        true|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|75166.66666666667|       false|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|75166.66666666667|        true|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|75166.66666666667|       false|\n+---+------+----------+----+------+----------+------------+--------------------+------------+-----------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_salary = (df.select(F.avg('salary')).first()[0])\n",
    "\n",
    "df.withColumns(\n",
    "    {\n",
    "        'avg_salary':F.lit(mean_salary),\n",
    "        'is_above_avg':F.col(\"salary\")>=mean_salary\n",
    "    }\n",
    ").show()\n",
    "\n",
    "\n",
    "# average_salary = (df.select(F.avg('salary')).first()[0])\n",
    " \n",
    "# df.withColumns(\n",
    "#     {\n",
    "#     'avg_salary': F.lit(average_salary),\n",
    "#     'is_above_avg': F.col('salary') >= average_salary}\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e62be71-391b-4b7e-aa64-2874fc572b99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ref_df=spark.read.format(\"parquet\").load(\"dbfs:///FileStore/tables/data/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b504b704-940c-4f2b-bb05-9702bcb78b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ref_df = ref_df.dropDuplicates([\"manager\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6c3aaf-8e86-4819-8058-bc91fb9a06ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n|department|manager|    lead|\n+----------+-------+--------+\n|        HR|   Brad|   Brian|\n|        IT|  Chris|Chandler|\n|  Delivery|   Leon|  Louise|\n|   Finance|  Megan|   Molly|\n+----------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "ref_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29350e8f-2cf0-4393-8766-9e509086683d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18cc3b74-dc5d-4b03-a5f6-95bd467860a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>id</th><th>name</th><th>dob</th><th>age</th><th>salary</th><th>salary_raise</th><th>salary_raise_10_perc</th><th>age_group</th><th>manager</th><th>lead</th></tr></thead><tbody><tr><td>IT</td><td>1</td><td>John</td><td>1992-05-12</td><td>32</td><td>70000</td><td>73500.0</td><td>70700.0</td><td>31 to 40</td><td>Chris</td><td>Chandler</td></tr><tr><td>Finance</td><td>4</td><td>Emily</td><td>1994-11-22</td><td>30</td><td>65000</td><td>68250.0</td><td>65650.0</td><td>21 to 30</td><td>Megan</td><td>Molly</td></tr><tr><td>HR</td><td>2</td><td>Alice</td><td>1997-02-28</td><td>27</td><td>60000</td><td>63000.0</td><td>60600.0</td><td>21 to 30</td><td>Brad</td><td>Brian</td></tr><tr><td>Finance</td><td>6</td><td>Susan</td><td>1989-07-05</td><td>35</td><td>75000</td><td>78750.0</td><td>75750.0</td><td>31 to 40</td><td>Megan</td><td>Molly</td></tr><tr><td>IT</td><td>7</td><td>Mike</td><td>1976-03-15</td><td>48</td><td>95000</td><td>99750.0</td><td>95950.0</td><td>More than 40</td><td>Chris</td><td>Chandler</td></tr><tr><td>IT</td><td>3</td><td>Bob</td><td>null</td><td>null</td><td>80000</td><td>84000.0</td><td>80800.0</td><td>More than 40</td><td>Chris</td><td>Chandler</td></tr><tr><td>HR</td><td>5</td><td>David</td><td>1981-12-18</td><td>43</td><td>90000</td><td>94500.0</td><td>90900.0</td><td>More than 40</td><td>Brad</td><td>Brian</td></tr><tr><td>Finance</td><td>10</td><td>Sophie</td><td>1992-06-30</td><td>32</td><td>62000</td><td>65100.0</td><td>62620.0</td><td>31 to 40</td><td>Megan</td><td>Molly</td></tr><tr><td>Finance</td><td>2</td><td>Alice</td><td>1997-02-28</td><td>27</td><td>90000</td><td>94500.0</td><td>90900.0</td><td>21 to 30</td><td>Megan</td><td>Molly</td></tr><tr><td>Finance</td><td>4</td><td>Emily</td><td>1994-11-22</td><td>30</td><td>70000</td><td>73500.0</td><td>70700.0</td><td>21 to 30</td><td>Megan</td><td>Molly</td></tr><tr><td>IT</td><td>9</td><td>James</td><td>1983-10-14</td><td>41</td><td>87000</td><td>91350.0</td><td>87870.0</td><td>More than 40</td><td>Chris</td><td>Chandler</td></tr><tr><td>HR</td><td>8</td><td>Lisa</td><td>1995-08-20</td><td>29</td><td>58000</td><td>60900.0</td><td>58580.0</td><td>21 to 30</td><td>Brad</td><td>Brian</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IT",
         1,
         "John",
         "1992-05-12",
         32,
         70000,
         73500.0,
         70700.0,
         "31 to 40",
         "Chris",
         "Chandler"
        ],
        [
         "Finance",
         4,
         "Emily",
         "1994-11-22",
         30,
         65000,
         68250.0,
         65650.0,
         "21 to 30",
         "Megan",
         "Molly"
        ],
        [
         "HR",
         2,
         "Alice",
         "1997-02-28",
         27,
         60000,
         63000.0,
         60600.0,
         "21 to 30",
         "Brad",
         "Brian"
        ],
        [
         "Finance",
         6,
         "Susan",
         "1989-07-05",
         35,
         75000,
         78750.0,
         75750.0,
         "31 to 40",
         "Megan",
         "Molly"
        ],
        [
         "IT",
         7,
         "Mike",
         "1976-03-15",
         48,
         95000,
         99750.0,
         95950.0,
         "More than 40",
         "Chris",
         "Chandler"
        ],
        [
         "IT",
         3,
         "Bob",
         null,
         null,
         80000,
         84000.0,
         80800.0,
         "More than 40",
         "Chris",
         "Chandler"
        ],
        [
         "HR",
         5,
         "David",
         "1981-12-18",
         43,
         90000,
         94500.0,
         90900.0,
         "More than 40",
         "Brad",
         "Brian"
        ],
        [
         "Finance",
         10,
         "Sophie",
         "1992-06-30",
         32,
         62000,
         65100.0,
         62620.0,
         "31 to 40",
         "Megan",
         "Molly"
        ],
        [
         "Finance",
         2,
         "Alice",
         "1997-02-28",
         27,
         90000,
         94500.0,
         90900.0,
         "21 to 30",
         "Megan",
         "Molly"
        ],
        [
         "Finance",
         4,
         "Emily",
         "1994-11-22",
         30,
         70000,
         73500.0,
         70700.0,
         "21 to 30",
         "Megan",
         "Molly"
        ],
        [
         "IT",
         9,
         "James",
         "1983-10-14",
         41,
         87000,
         91350.0,
         87870.0,
         "More than 40",
         "Chris",
         "Chandler"
        ],
        [
         "HR",
         8,
         "Lisa",
         "1995-08-20",
         29,
         58000,
         60900.0,
         58580.0,
         "21 to 30",
         "Brad",
         "Brian"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "salary_raise",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "salary_raise_10_perc",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "age_group",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "manager",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lead",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.join(\n",
    "    ref_df,\n",
    "    \"department\",\n",
    "    \"left\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ecdffa-cd9e-4f70-9e64-1af12377afdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5947944-175d-4d3a-a26e-da4a6dca0340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+----------+-------+--------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|department|manager|    lead|\n+---+------+----------+----+------+----------+------------+--------------------+------------+----------+-------+--------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|        IT|  Chris|Chandler|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|   Finance|  Megan|   Molly|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|        HR|   Brad|   Brian|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|   Finance|  Megan|   Molly|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|        IT|  Chris|Chandler|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|        IT|  Chris|Chandler|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|        HR|   Brad|   Brian|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|   Finance|  Megan|   Molly|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|   Finance|  Megan|   Molly|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|   Finance|  Megan|   Molly|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|        IT|  Chris|Chandler|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|        HR|   Brad|   Brian|\n+---+------+----------+----+------+----------+------------+--------------------+------------+----------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(\n",
    "    ref_df,\n",
    "    df.department==ref_df.department,\n",
    "    \"left\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7415fdbd-40a3-4ce8-97b6-edad43c6a667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.alias(\"main\")\n",
    "ref_df=ref_df.alias(\"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c194f6d-66aa-487e-a75c-c1e7b6c3badd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|\n+---+------+----------+----+------+----------+------------+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(\n",
    "    ref_df,\n",
    "    F.col(\"main.department\")==F.col(\"reference.department\"),\n",
    "    \"left\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b93172e-9c8c-4a78-aa51-1d6aee61a547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4331949038293567>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[1;32m      2\u001B[0m     ref_df\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmanager\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlead\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m          F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference_department\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \n",
       "\u001B[1;32m      7\u001B[0m     ),\n",
       "\u001B[1;32m      8\u001B[0m     F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain.department\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m==\u001B[39mF\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference.department\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m )\n",
       "\u001B[1;32m     11\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n",
       "\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n",
       "\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"reference\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\"."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4331949038293567>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m      2\u001B[0m     ref_df\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmanager\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlead\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m          F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference_department\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m     ),\n\u001B[1;32m      8\u001B[0m     F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain.department\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m==\u001B[39mF\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreference.department\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     11\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"reference\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"reference\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRING\".",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.join(\n",
    "    ref_df.select(\n",
    "        \"manager\",\n",
    "        \"lead\",\n",
    "         F.col(\"department\").alias(\"reference_department\").alias(\"reference\")\n",
    "\n",
    "    ),\n",
    "    F.col(\"main.department\")==F.col(\"reference.department\"),\n",
    "    \"left\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f1b58d-689f-48da-99f6-88c9dfa82d23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+--------------------+------------+-------+--------+\n| id|  name|       dob| age|salary|department|salary_raise|salary_raise_10_perc|   age_group|manager|    lead|\n+---+------+----------+----+------+----------+------------+--------------------+------------+-------+--------+\n|  1|  John|1992-05-12|  32| 70000|        IT|     73500.0|             70700.0|    31 to 40|  Chris|Chandler|\n|  4| Emily|1994-11-22|  30| 65000|   Finance|     68250.0|             65650.0|    21 to 30|  Megan|   Molly|\n|  2| Alice|1997-02-28|  27| 60000|        HR|     63000.0|             60600.0|    21 to 30|   Brad|   Brian|\n|  6| Susan|1989-07-05|  35| 75000|   Finance|     78750.0|             75750.0|    31 to 40|  Megan|   Molly|\n|  7|  Mike|1976-03-15|  48| 95000|        IT|     99750.0|             95950.0|More than 40|  Chris|Chandler|\n|  3|   Bob|      null|null| 80000|        IT|     84000.0|             80800.0|More than 40|  Chris|Chandler|\n|  5| David|1981-12-18|  43| 90000|        HR|     94500.0|             90900.0|More than 40|   Brad|   Brian|\n| 10|Sophie|1992-06-30|  32| 62000|   Finance|     65100.0|             62620.0|    31 to 40|  Megan|   Molly|\n|  2| Alice|1997-02-28|  27| 90000|   Finance|     94500.0|             90900.0|    21 to 30|  Megan|   Molly|\n|  4| Emily|1994-11-22|  30| 70000|   Finance|     73500.0|             70700.0|    21 to 30|  Megan|   Molly|\n|  9| James|1983-10-14|  41| 87000|        IT|     91350.0|             87870.0|More than 40|  Chris|Chandler|\n|  8|  Lisa|1995-08-20|  29| 58000|        HR|     60900.0|             58580.0|    21 to 30|   Brad|   Brian|\n+---+------+----------+----+------+----------+------------+--------------------+------------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df.alias(\"main\").join(\n",
    "    ref_df.alias(\"reference\"),\n",
    "    F.col(\"main.department\")==F.col(\"reference.department\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    F.col(\"main.*\"),\n",
    "    F.col(\"reference.manager\"),\n",
    "    F.col(\"lead\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d50cfe8-fd54-45ee-b151-928c8d3af90f",
     "showTitle": true,
     "title": "Load(Save) Data"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"csv\").save(\"dbfs:/FileStore/tables/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054091ee-3043-48d3-814b-d2726df338a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/final/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1707888306000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_632594348943999244</td><td>_committed_632594348943999244</td><td>212</td><td>1707888305000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_6588261513969403078</td><td>_committed_6588261513969403078</td><td>114</td><td>1707804417000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_vacuum5112769831977745175</td><td>_committed_vacuum5112769831977745175</td><td>96</td><td>1707888306000</td></tr><tr><td>dbfs:/FileStore/tables/final/_started_632594348943999244</td><td>_started_632594348943999244</td><td>0</td><td>1707888305000</td></tr><tr><td>dbfs:/FileStore/tables/final/part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv</td><td>part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv</td><td>698</td><td>1707888305000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/final/_SUCCESS",
         "_SUCCESS",
         0,
         1707888306000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_632594348943999244",
         "_committed_632594348943999244",
         212,
         1707888305000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_6588261513969403078",
         "_committed_6588261513969403078",
         114,
         1707804417000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_vacuum5112769831977745175",
         "_committed_vacuum5112769831977745175",
         96,
         1707888306000
        ],
        [
         "dbfs:/FileStore/tables/final/_started_632594348943999244",
         "_started_632594348943999244",
         0,
         1707888305000
        ],
        [
         "dbfs:/FileStore/tables/final/part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv",
         "part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv",
         698,
         1707888305000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables/final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d70782-4d4d-486c-8ce8-9dceec539ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e1a9e5-db70-4a1b-bd56-17b9aac4c971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca419589-6f87-4e0a-8e99-87b84f5ed18c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe28e06-e774-49dc-9ae7-d39ea8cc8cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4331949038293571>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-4331949038293571>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0ICogZnJvbSBgZmluYWxg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [final], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4331949038293571>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-4331949038293571>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0ICogZnJvbSBgZmluYWxg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [final], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [final], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "select * from `final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a4fe7d-ba0d-4356-b87b-a5667155eedb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:677)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:1)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:43)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:45)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:47)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw.&lt;init&gt;(command-4331949038293572:49)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw.&lt;init&gt;(command-4331949038293572:51)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read.&lt;init&gt;(command-4331949038293572:53)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$.&lt;init&gt;(command-4331949038293572:57)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$.&lt;clinit&gt;(command-4331949038293572)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:656)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:677)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:651)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:1)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:43)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:45)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw$$iw.&lt;init&gt;(command-4331949038293572:47)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw$$iw.&lt;init&gt;(command-4331949038293572:49)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$$iw.&lt;init&gt;(command-4331949038293572:51)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read.&lt;init&gt;(command-4331949038293572:53)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$.&lt;init&gt;(command-4331949038293572:57)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$read$.&lt;clinit&gt;(command-4331949038293572)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval$.$print(&lt;notebook&gt;:6)\n\tat $linec3a8518b495d4fad8923e0aedc76306e31.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:227)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1283)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1236)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:227)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:889)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$21(DriverLocal.scala:872)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:849)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:660)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:652)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:571)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:606)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:448)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:389)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:247)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /FileStore/tables/final/delta",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables/final/delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012a1bbb-75f7-4730-ac7b-b452cbc470c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/final/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1707888306000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_632594348943999244</td><td>_committed_632594348943999244</td><td>212</td><td>1707888305000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_6588261513969403078</td><td>_committed_6588261513969403078</td><td>114</td><td>1707804417000</td></tr><tr><td>dbfs:/FileStore/tables/final/_committed_vacuum5112769831977745175</td><td>_committed_vacuum5112769831977745175</td><td>96</td><td>1707888306000</td></tr><tr><td>dbfs:/FileStore/tables/final/_started_632594348943999244</td><td>_started_632594348943999244</td><td>0</td><td>1707888305000</td></tr><tr><td>dbfs:/FileStore/tables/final/part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv</td><td>part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv</td><td>698</td><td>1707888305000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/final/_SUCCESS",
         "_SUCCESS",
         0,
         1707888306000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_632594348943999244",
         "_committed_632594348943999244",
         212,
         1707888305000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_6588261513969403078",
         "_committed_6588261513969403078",
         114,
         1707804417000
        ],
        [
         "dbfs:/FileStore/tables/final/_committed_vacuum5112769831977745175",
         "_committed_vacuum5112769831977745175",
         96,
         1707888306000
        ],
        [
         "dbfs:/FileStore/tables/final/_started_632594348943999244",
         "_started_632594348943999244",
         0,
         1707888305000
        ],
        [
         "dbfs:/FileStore/tables/final/part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv",
         "part-00000-tid-632594348943999244-e965766c-a0ef-4c54-bfd5-56eddafe23cb-205-1-c000.csv",
         698,
         1707888305000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables/final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b04503-cf8c-44c6-b8dd-c7adfc0e975e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2881926381412166>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-2881926381412166>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mZGVzYyBgZmluYWxg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 5;\n",
       "'DescribeRelation false, [col_name#85, data_type#86, comment#87]\n",
       "+- 'UnresolvedTableOrView [final], DESCRIBE TABLE, true\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2881926381412166>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-2881926381412166>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mZGVzYyBgZmluYWxg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 5;\n'DescribeRelation false, [col_name#85, data_type#86, comment#87]\n+- 'UnresolvedTableOrView [final], DESCRIBE TABLE, true\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `final` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 5;\n'DescribeRelation false, [col_name#85, data_type#86, comment#87]\n+- 'UnresolvedTableOrView [final], DESCRIBE TABLE, true\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "desc `final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220e25e5-a460-4019-86ec-d273b8b2764e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4331949038293573,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark  joins and select",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
