{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad2fd38-68bf-4977-bda1-10f264fb604e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/data.zip\", recurse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6651ea91-e0e3-426e-98be-5fb201752a1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2342463238466600#setting/sparkui/0213-044427-wwyq4l92/driver-4681814273899057300\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2342463238466600#setting/sparkui/0213-044427-wwyq4l92/driver-4681814273899057300\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094e25ea-005a-451d-977f-f248758e6798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/csv/</td><td>csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/data.zip</td><td>data.zip</td><td>2274</td><td>1707719091000</td></tr><tr><td>dbfs:/FileStore/tables/json/</td><td>json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/parquet/</td><td>parquet/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/csv/",
         "csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/data.zip",
         "data.zip",
         2274,
         1707719091000
        ],
        [
         "dbfs:/FileStore/tables/json/",
         "json/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/parquet/",
         "parquet/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a06bd490-b9e9-4e9f-83a7-cc1e660a12e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\").option(\"header\",True).load(\"dbfs:///FileStore/tables/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed77989-0f42-44bb-9460-e2b65666ea38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").option(\"header\",True).load(\"dbfs:///FileStore/tables/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773a00d0-175b-41bc-9868-39a783afd138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, IntegerType,StringType,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736af9a3-29fe-4e2d-8c28-f4aefc63e149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"dob\", DateType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"salary\", IntegerType()),\n",
    "    StructField(\"department\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21733a2-9879-4cd8-b0a0-1b967cde20ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\").schema(schema).option(\"header\",True).load(\"dbfs:///FileStore/tables/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5a5f0f-9128-431e-bfd7-db90270adc04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").schema(schema).option(\"header\",True).load(\"dbfs:///FileStore/tables/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad55f5cd-0601-4b75-9458-16f249df848a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca9d4d4-e89c-4c42-86eb-9083c2e4d637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+------+----------+\n| id| name|       dob| age|salary|department|\n+---+-----+----------+----+------+----------+\n|  1| John|1992-05-12|  30| 70000|        IT|\n|  2|Alice|1997-02-28|  25| 60000|        HR|\n|  3|  Bob|      null|null| 80000|        IT|\n|  4|Emily|1994-11-22|  28| 65000|   Finance|\n+---+-----+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6a666e-9451-4c8b-bf44-30678b496af5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396f38dc-7f4c-4511-beea-a0d5aaba0fee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  9| James|1983-10-14|  39| 87000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e98cd4e-6498-417a-9820-cad849e500d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PySpark Task - 1 :  \n",
    "Load both csv and json union them properly. Instead of dropping duplicates, create a boolean column is_duplicate and set False to only one row and True to rest of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ef42dc-3238-4eb0-92a1-7512590aed93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b996bd0e-1ff1-4596-bc5e-35c835c6cfd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_csv.union(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b4dbe6-031f-412a-9929-a437a79ea01a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  9| James|1983-10-14|  39| 87000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fa165b-1416-4bd8-acf4-4d1ce3cb6d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+\n| id|  name|       dob| age|salary|department|is_duplicate|\n+---+------+----------+----+------+----------+------------+\n|  1|  John|1992-05-12|  30| 70000|        IT|       false|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       false|\n|  2| Alice|1997-02-28|  25| 60000|        HR|        true|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|       false|\n|  3|   Bob|      null|null| 80000|        IT|       false|\n|  3|   Bob|      null|null| 80000|        IT|        true|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       false|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|        true|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|       false|\n|  5| David|1981-12-18|  41| 90000|        HR|       false|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|       false|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|       false|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|       false|\n|  9| James|1983-10-14|  39| 87000|        IT|       false|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|       false|\n+---+------+----------+----+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"is_duplicate\",\n",
    "    (f.row_number().over(Window.partitionBy(df.columns).orderBy(\"id\"))) > 1\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874ba9c5-2a8f-400c-bfaa-f2592d17e70f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PySpark Task - 2 :  \n",
    "A. Calculate mean salary and check if it is greater or equal to the salary of employees in each department.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eec939d-bc6f-4bfa-828f-8b931ab29e8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  9| James|1983-10-14|  39| 87000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef974d5d-74d1-4ddb-bcb5-dbd8be2e6f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+-----------+\n| id|  name|       dob| age|salary|department|mean_salary|\n+---+------+----------+----+------+----------+-----------+\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       true|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       true|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|      false|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|       true|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|      false|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|       true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       true|\n|  5| David|1981-12-18|  41| 90000|        HR|      false|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|       true|\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n|  3|   Bob|      null|null| 80000|        IT|      false|\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n|  3|   Bob|      null|null| 80000|        IT|      false|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|      false|\n|  9| James|1983-10-14|  39| 87000|        IT|      false|\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n+---+------+----------+----+------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"mean_salary\",\n",
    "    (f.avg(\"salary\").over(Window.partitionBy(\"department\")) > df.salary\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96715496-7045-484b-8720-503e6967b963",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PySpark Task - 2 :  \n",
    "B. Calculate mean salary and check if it is greater or equal to the salary of all employees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fdcc4b4-286d-48f2-acdd-8dd3282d68cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+-----------+\n| id|  name|       dob| age|salary|department|mean_salary|\n+---+------+----------+----+------+----------+-----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       true|\n|  3|   Bob|      null|null| 80000|        IT|      false|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       true|\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       true|\n|  3|   Bob|      null|null| 80000|        IT|      false|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       true|\n|  5| David|1981-12-18|  41| 90000|        HR|      false|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|      false|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|      false|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|       true|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|      false|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|       true|\n|  9| James|1983-10-14|  39| 87000|        IT|      false|\n|  1|  John|1992-05-12|  30| 70000|        IT|       true|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|       true|\n+---+------+----------+----+------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"mean_salary\",\n",
    "    (f.avg(\"salary\").over(Window.partitionBy()) > df.salary\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c34b5e1c-d971-4374-b979-60536da05105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4376117134188860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Tasks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
