{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a18ac48-2f96-4523-8e40-d07ceb7833c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2085830725623723#setting/sparkui/0213-045454-c88vdxq/driver-6707261001432822827\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2085830725623723#setting/sparkui/0213-045454-c88vdxq/driver-6707261001432822827\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd464659-0695-42bf-9fc5-dcaa3dbdf581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/csv/</td><td>csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/json/</td><td>json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/parquet/</td><td>parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/train/</td><td>train/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/csv/",
         "csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/json/",
         "json/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/parquet/",
         "parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/train/",
         "train/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/FileStore/tables/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11df0c75-8bb6-4b8b-8ae6-c181d4954598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\").option(\"header\",True).load(\"dbfs:/FileStore/tables/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967a8b7f-ecf0-4c17-a048-4a22e46172d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[90]: pyspark.sql.dataframe.DataFrame"
     ]
    }
   ],
   "source": [
    "type(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039ba377-3039-4281-a484-f8a31e84e49f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- age: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#df_csv.show()\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c3df4b-ad3f-46ca-bcc5-5abe64217a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType, StringType,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a792e94-bb24-4065-9173-8b8ed30a69a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sch = StructType([\n",
    "    StructField(\"id\",IntegerType()),\n",
    "    StructField(\"name\",StringType()),\n",
    "    StructField(\"dob\",DateType()),\n",
    "    StructField(\"age\",IntegerType()),\n",
    "    StructField(\"salary\",IntegerType()),\n",
    "    StructField(\"department\",StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8b44aa-e734-419a-9a52-baa270487c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\").schema(sch).option(\"header\",True).load(\"dbfs:/FileStore/tables/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc49fc16-b605-49dd-9a46-3ed28017bdc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+------+----------+\n| id| name|       dob| age|salary|department|\n+---+-----+----------+----+------+----------+\n|  1| John|1992-05-12|  30| 70000|        IT|\n|  2|Alice|1997-02-28|  25| 60000|        HR|\n|  3|  Bob|      null|null| 80000|        IT|\n|  4|Emily|1994-11-22|  28| 65000|   Finance|\n+---+-----+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f5ea5c-dca5-4cb5-9756-f5a854f02cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71b416a-fcf4-458d-bea8-eb255c1262e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").load(\"dbfs:/FileStore/tables/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85537cb9-4f07-4852-a7b1-a8d27303a4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+---+------+------+\n| age|department|       dob| id|  name|salary|\n+----+----------+----------+---+------+------+\n|  30|        IT|1992-05-12|  1|  John| 70000|\n|  25|        HR|1997-02-28|  2| Alice| 60000|\n|null|        IT|      null|  3|   Bob| 80000|\n|  28|   Finance|1994-11-22|  4| Emily| 65000|\n|  41|        HR|1981-12-18|  5| David| 90000|\n|  33|   Finance|1989-07-05|  6| Susan| 75000|\n|  46|        IT|1976-03-15|  7|  Mike| 95000|\n|  30|        IT|1992-05-12|  1|  John| 70000|\n|  25|        HR|1997-02-28|  2| Alice| 60000|\n|null|        IT|      null|  3|   Bob| 80000|\n|  28|   Finance|1994-11-22|  4| Emily| 65000|\n|  41|        HR|1981-12-18|  5| David| 90000|\n|  33|   Finance|1989-07-05|  6| Susan| 75000|\n|  46|        IT|1976-03-15|  7|  Mike| 95000|\n|  30|   Finance|1992-06-30| 10|Sophie| 62000|\n|  30|   Finance|1992-06-30| 10|Sophie| 62000|\n|  25|   Finance|1997-02-28|  2| Alice| 90000|\n|  25|   Finance|1997-02-28|  2| Alice| 90000|\n|  28|   Finance|1994-11-22|  4| Emily| 70000|\n|  28|   Finance|1994-11-22|  4| Emily| 70000|\n+----+----------+----------+---+------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468c58c8-c679-4503-8bb4-cb1365532a86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- department: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df007469-73bf-4148-a50e-4b1439cda71a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+----+------+----------+\n|  id|   name|       dob| age|salary|department|\n+----+-------+----------+----+------+----------+\n|   2|  Alice|1997-02-28|  25| 60000|        HR|\n|   3|    Bob|      null|null| 80000|        IT|\n|   4|  Emily|1994-11-22|  28| 65000|   Finance|\n|   1|   John|1992-05-12|  30| 70000|        IT|\n|null|     IT|      null|   3|   Bob|     80000|\n|  33|Finance|1989-07-05|   6| Susan|     75000|\n|  41|     HR|1981-12-18|   5| David|     90000|\n|  25|     HR|1997-02-28|   2| Alice|     60000|\n|  30|     IT|1992-05-12|   1|  John|     70000|\n|  28|Finance|1994-11-22|   4| Emily|     65000|\n|  46|     IT|1976-03-15|   7|  Mike|     95000|\n|  30|Finance|1992-06-30|  10|Sophie|     62000|\n|  25|Finance|1997-02-28|   2| Alice|     90000|\n|  28|Finance|1994-11-22|   4| Emily|     70000|\n|  39|     IT|1983-10-14|   9| James|     87000|\n|  27|     HR|1995-08-20|   8|  Lisa|     58000|\n+----+-------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df_csv.union(df_json)\n",
    "\n",
    "df1=df.dropDuplicates()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5662f8-a599-4483-ac67-06888723345d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'department', 'dob', 'id', 'name', 'salary'] ['id', 'name', 'dob', 'age', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "print(df_json.columns,df_csv.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcb580b-4d68-472b-b3d2-601ee3efd0c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+----+------+----------+\n|  id|   name|       dob| age|salary|department|\n+----+-------+----------+----+------+----------+\n|   1|   John|1992-05-12|  30| 70000|        IT|\n|   2|  Alice|1997-02-28|  25| 60000|        HR|\n|   3|    Bob|      null|null| 80000|        IT|\n|   4|  Emily|1994-11-22|  28| 65000|   Finance|\n|  30|     IT|1992-05-12|   1|  John|     70000|\n|  25|     HR|1997-02-28|   2| Alice|     60000|\n|null|     IT|      null|   3|   Bob|     80000|\n|  28|Finance|1994-11-22|   4| Emily|     65000|\n|  41|     HR|1981-12-18|   5| David|     90000|\n|  33|Finance|1989-07-05|   6| Susan|     75000|\n|  46|     IT|1976-03-15|   7|  Mike|     95000|\n|  30|     IT|1992-05-12|   1|  John|     70000|\n|  25|     HR|1997-02-28|   2| Alice|     60000|\n|null|     IT|      null|   3|   Bob|     80000|\n|  28|Finance|1994-11-22|   4| Emily|     65000|\n|  41|     HR|1981-12-18|   5| David|     90000|\n|  33|Finance|1989-07-05|   6| Susan|     75000|\n|  46|     IT|1976-03-15|   7|  Mike|     95000|\n|  30|Finance|1992-06-30|  10|Sophie|     62000|\n|  30|Finance|1992-06-30|  10|Sophie|     62000|\n+----+-------+----------+----+------+----------+\nonly showing top 20 rows\n\n30\n"
     ]
    }
   ],
   "source": [
    "df_json = df_json.select(df_csv.columns)\n",
    "df.show()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618fd66f-b4e0-4ee0-8de5-2a3d371ab5a1",
     "showTitle": true,
     "title": "Transformation:"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 60000|  25|\n| 80000|null|\n| 65000|  28|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|Sophie|  10|\n|Sophie|  10|\n+------+----+\nonly showing top 20 rows\n\n+-------+\n|   name|\n+-------+\n|   John|\n|  Alice|\n|    Bob|\n|  Emily|\n|     IT|\n|     HR|\n|     IT|\n|Finance|\n|     HR|\n|Finance|\n|     IT|\n|     IT|\n|     HR|\n|     IT|\n|Finance|\n|     HR|\n|Finance|\n|     IT|\n|Finance|\n|Finance|\n+-------+\nonly showing top 20 rows\n\n+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 60000|  25|\n| 80000|null|\n| 65000|  28|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|Sophie|  10|\n|Sophie|  10|\n+------+----+\nonly showing top 20 rows\n\n+------+----+\n|salary| age|\n+------+----+\n| 70000|  30|\n| 60000|  25|\n| 80000|null|\n| 65000|  28|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|  John|   1|\n| Alice|   2|\n|   Bob|   3|\n| Emily|   4|\n| David|   5|\n| Susan|   6|\n|  Mike|   7|\n|Sophie|  10|\n|Sophie|  10|\n+------+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    'salary',\n",
    "    'age'\n",
    ").show()\n",
    "df.select(\n",
    "    \"name\"\n",
    "\n",
    ").show()\n",
    "df.select(\n",
    "    ['salary','age']\n",
    ").show()\n",
    "df.select(\n",
    "    df.salary,\n",
    "    df.age,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bead945-3243-48ac-bf03-9340b3af9ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql functions import *\n",
    "#from pyspark.sql functions import col,min,max\n",
    "from pyspark.sql import functions as f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5268a700-fb6f-4431-a866-4edfce5739b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1900797974329514>:8\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# df.select(\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#     df.salary + .05*df.salary,\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#     f.year(f.current_timestamp())-f.year(\"dob\"),\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#     f.year(f.current_timestamp())-f.year(f.col(\"dob\")).alias(\"age\")\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# ).show()\u001B[39;00m\n",
       "\u001B[0;32m----> 8\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msalary\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m.05\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msalary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdf.salary + .05*df.salary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msalaryRaised\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43myear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_timestamp\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43myear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdob\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `df`.`salary` cannot be resolved. Did you mean one of the following? [`salary`, `age`, `dob`, `name`, `department`].; line 1 pos 0;\n",
       "'Project [(cast(salary#1857 as double) + (cast(salary#1857 as double) * 0.05)) AS (salary + (salary * 0.05))#2026, ('df.salary + (0.05 * 'df.salary)) AS salaryRaised#2024, (year(cast(current_timestamp() as date)) - year(cast(dob#1855 as date))) AS age#2025]\n",
       "+- Union false, false\n",
       "   :- Project [cast(id#1758 as bigint) AS id#1854L, name#1759, cast(dob#1760 as string) AS dob#1855, cast(age#1761 as bigint) AS age#1856L, cast(salary#1762 as string) AS salary#1857, department#1763]\n",
       "   :  +- Relation [id#1758,name#1759,dob#1760,age#1761,salary#1762,department#1763] csv\n",
       "   +- Project [age#1810L, department#1811, dob#1812, id#1813L, name#1814, cast(salary#1815L as string) AS salary#1858]\n",
       "      +- Relation [age#1810L,department#1811,dob#1812,id#1813L,name#1814,salary#1815L] json\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1900797974329514>:8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# df.select(\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#     df.salary + .05*df.salary,\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#     f.year(f.current_timestamp())-f.year(\"dob\"),\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#     f.year(f.current_timestamp())-f.year(f.col(\"dob\")).alias(\"age\")\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# ).show()\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msalary\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m.05\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msalary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdf.salary + .05*df.salary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msalaryRaised\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43myear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_timestamp\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43myear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdob\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `df`.`salary` cannot be resolved. Did you mean one of the following? [`salary`, `age`, `dob`, `name`, `department`].; line 1 pos 0;\n'Project [(cast(salary#1857 as double) + (cast(salary#1857 as double) * 0.05)) AS (salary + (salary * 0.05))#2026, ('df.salary + (0.05 * 'df.salary)) AS salaryRaised#2024, (year(cast(current_timestamp() as date)) - year(cast(dob#1855 as date))) AS age#2025]\n+- Union false, false\n   :- Project [cast(id#1758 as bigint) AS id#1854L, name#1759, cast(dob#1760 as string) AS dob#1855, cast(age#1761 as bigint) AS age#1856L, cast(salary#1762 as string) AS salary#1857, department#1763]\n   :  +- Relation [id#1758,name#1759,dob#1760,age#1761,salary#1762,department#1763] csv\n   +- Project [age#1810L, department#1811, dob#1812, id#1813L, name#1814, cast(salary#1815L as string) AS salary#1858]\n      +- Relation [age#1810L,department#1811,dob#1812,id#1813L,name#1814,salary#1815L] json\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `df`.`salary` cannot be resolved. Did you mean one of the following? [`salary`, `age`, `dob`, `name`, `department`].; line 1 pos 0;\n'Project [(cast(salary#1857 as double) + (cast(salary#1857 as double) * 0.05)) AS (salary + (salary * 0.05))#2026, ('df.salary + (0.05 * 'df.salary)) AS salaryRaised#2024, (year(cast(current_timestamp() as date)) - year(cast(dob#1855 as date))) AS age#2025]\n+- Union false, false\n   :- Project [cast(id#1758 as bigint) AS id#1854L, name#1759, cast(dob#1760 as string) AS dob#1855, cast(age#1761 as bigint) AS age#1856L, cast(salary#1762 as string) AS salary#1857, department#1763]\n   :  +- Relation [id#1758,name#1759,dob#1760,age#1761,salary#1762,department#1763] csv\n   +- Project [age#1810L, department#1811, dob#1812, id#1813L, name#1814, cast(salary#1815L as string) AS salary#1858]\n      +- Relation [age#1810L,department#1811,dob#1812,id#1813L,name#1814,salary#1815L] json\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df.select(\n",
    "#     df.salary + .05*df.salary,\n",
    "#     f.year(f.current_timestamp())-f.year(\"dob\"),\n",
    "#     f.year(f.current_timestamp())-f.year(f.col(\"dob\")).alias(\"age\")\n",
    "\n",
    "# ).show()\n",
    "\n",
    "df.select(\n",
    "    (df.salary + .05*df.salary),\n",
    "    f.expr(\"df.salary + .05*df.salary\").alias(\"salaryRaised\"),\n",
    "    (\n",
    "        f.year(f.current_timestamp())-f.year(f.col(\"dob\"))\n",
    "    ).alias(\"age\")\n",
    ").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e32eef8-2e91-4d0f-90c2-0ac0341e30c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+----+------+----------+-------------+\n|  id|   name|       dob| age|salary|department|salary raised|\n+----+-------+----------+----+------+----------+-------------+\n|   1|   John|1992-05-12|  30| 70000|        IT|      77000.0|\n|   2|  Alice|1997-02-28|  25| 60000|        HR|      66000.0|\n|   3|    Bob|      null|null| 80000|        IT|      88000.0|\n|   4|  Emily|1994-11-22|  28| 65000|   Finance|      71500.0|\n|  30|     IT|1992-05-12|   1|  John|     70000|         null|\n|  25|     HR|1997-02-28|   2| Alice|     60000|         null|\n|null|     IT|      null|   3|   Bob|     80000|         null|\n|  28|Finance|1994-11-22|   4| Emily|     65000|         null|\n|  41|     HR|1981-12-18|   5| David|     90000|         null|\n|  33|Finance|1989-07-05|   6| Susan|     75000|         null|\n|  46|     IT|1976-03-15|   7|  Mike|     95000|         null|\n|  30|     IT|1992-05-12|   1|  John|     70000|         null|\n|  25|     HR|1997-02-28|   2| Alice|     60000|         null|\n|null|     IT|      null|   3|   Bob|     80000|         null|\n|  28|Finance|1994-11-22|   4| Emily|     65000|         null|\n|  41|     HR|1981-12-18|   5| David|     90000|         null|\n|  33|Finance|1989-07-05|   6| Susan|     75000|         null|\n|  46|     IT|1976-03-15|   7|  Mike|     95000|         null|\n|  30|Finance|1992-06-30|  10|Sophie|     62000|         null|\n|  30|Finance|1992-06-30|  10|Sophie|     62000|         null|\n+----+-------+----------+----+------+----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\n",
    "    \"salary raised\",\n",
    "    (f.col(\"salary\")+.1*f.col(\"salary\"))\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d14a56-cc67-43fd-ac3b-b2264eb23bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-1900797974329516>:4\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"age\": f.year(f.current_timestamp()) - f.year(f.col(\"dob\"))\u001B[0m\n",
       "\u001B[0m         ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-1900797974329516>:4\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"age\": f.year(f.current_timestamp()) - f.year(f.col(\"dob\"))\u001B[0m\n\u001B[0m         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-1900797974329516>, line 4)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df.withColumns(\n",
    "    {\n",
    "        \"salary raised\":(f.col(\"salary\")+.1*f.col(\"salary\"),\n",
    "        \"age\": f.year(f.current_timestamp()) - f.year(f.col(\"dob\")) \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e96f61-177d-4d6b-ba25-5292ca891857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= df.withColumn(\n",
    "    \"age group\"\n",
    "    f.when(\n",
    "        f.col(\"age\")<=20,\n",
    "        \"Upto 20\"\n",
    "    ).when(\n",
    "        (f.col(\"age\")> 20) & (f.col(\"age\")<=30) ,\n",
    "        \"21 to 30\"\n",
    "\n",
    "    ).when(\n",
    "      (f.col(\"age\")> 30) & (f.col(\"age\")<=40) ,\n",
    "        \"31 to 40\"  \n",
    "    ).otherwise(\n",
    "        \"more than 40\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bcf46bb-5e4b-4edf-a411-39e40dd8d0f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df.drop(\"company\",\"has_dob\")\n",
    "df=df.withColumnRenamed(\"name\",\"first_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24047129-63da-48e3-94b7-a6ba67501964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_df = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/parquet\")\n",
    "ref_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ff12e4-43b1-4ede-b25a-7a8cd0142c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3130358376213347>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m(\n",
       "\u001B[1;32m      2\u001B[0m     ref_df,\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m \n",
       "\u001B[1;32m      6\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'join'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3130358376213347>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m(\n\u001B[1;32m      2\u001B[0m     ref_df,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m )\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'join'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'join'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.join(\n",
    "    ref_df,\n",
    "    \"department\",\n",
    "    \"left\",\n",
    "\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9737827-825f-4214-8c80-086699ad5b20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = df.join(\n",
    "#     ref_df,\n",
    "#     df.department == ref_df.department,\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9911044a-dff7-42f1-b854-e4e58b2dca83",
     "showTitle": true,
     "title": "load , save data::"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").format(\"csv\").save(\"dbfs:/FileStore/tables/newfinal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6c9080-1ada-4644-86ba-68f993049f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/FileStore/tables/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053ac02b-dabf-4690-94ec-ae02da07e8cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3884137933029565>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewfinal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3884137933029565>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnewfinal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'write'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'write'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"newfinal\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3130358376213350,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Day1,2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
