{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ccbbe3-8e80-4af5-8cca-e0e7b3c8d39f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4056554888253943#setting/sparkui/0220-120510-dt9qsjmi/driver-1630866416551147894\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4056554888253943#setting/sparkui/0220-120510-dt9qsjmi/driver-1630866416551147894\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a72315a-479b-4804-83b7-98a286b6ece6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/json/10_20220101.json</td><td>10_20220101.json</td><td>102</td><td>1707460794000</td></tr><tr><td>dbfs:/FileStore/tables/json/1_20220101.json</td><td>1_20220101.json</td><td>94</td><td>1707460794000</td></tr><tr><td>dbfs:/FileStore/tables/json/2_20220101.json</td><td>2_20220101.json</td><td>100</td><td>1707460795000</td></tr><tr><td>dbfs:/FileStore/tables/json/4_20220101.json</td><td>4_20220101.json</td><td>100</td><td>1707460795000</td></tr><tr><td>dbfs:/FileStore/tables/json/8_20220101.json</td><td>8_20220101.json</td><td>94</td><td>1707460795000</td></tr><tr><td>dbfs:/FileStore/tables/json/9_20220101.json</td><td>9_20220101.json</td><td>95</td><td>1707460795000</td></tr><tr><td>dbfs:/FileStore/tables/json/batch.jsonl</td><td>batch.jsonl</td><td>671</td><td>1707460795000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/json/10_20220101.json",
         "10_20220101.json",
         102,
         1707460794000
        ],
        [
         "dbfs:/FileStore/tables/json/1_20220101.json",
         "1_20220101.json",
         94,
         1707460794000
        ],
        [
         "dbfs:/FileStore/tables/json/2_20220101.json",
         "2_20220101.json",
         100,
         1707460795000
        ],
        [
         "dbfs:/FileStore/tables/json/4_20220101.json",
         "4_20220101.json",
         100,
         1707460795000
        ],
        [
         "dbfs:/FileStore/tables/json/8_20220101.json",
         "8_20220101.json",
         94,
         1707460795000
        ],
        [
         "dbfs:/FileStore/tables/json/9_20220101.json",
         "9_20220101.json",
         95,
         1707460795000
        ],
        [
         "dbfs:/FileStore/tables/json/batch.jsonl",
         "batch.jsonl",
         671,
         1707460795000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:///FileStore/tables/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c66236-d447-43ae-8fbb-b21526a6d04c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8868bcd0-a3bd-41d8-8895-5066720816ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sch = StructType([\n",
    "    StructField(\"id\",IntegerType()),\n",
    "    StructField(\"name\",StringType()),\n",
    "    StructField(\"dob\",DateType()),\n",
    "    StructField(\"age\",IntegerType()),\n",
    "    StructField(\"salary\",IntegerType()),\n",
    "    StructField(\"department\",StringType()),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751644e3-24a6-4068-9d4c-62e4bbaba8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.format(\"csv\").schema(sch).option(\"header\", True).load(\"dbfs:/FileStore/tables/csv/batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f9bbca-2262-4db3-8d20-63ef6b7aeb5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1155832-59c7-4614-9a92-ecf484e8cfb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+---+------+------+\n| age|department|       dob| id|  name|salary|\n+----+----------+----------+---+------+------+\n|  30|        IT|1992-05-12|  1|  John| 70000|\n|  25|        HR|1997-02-28|  2| Alice| 60000|\n|null|        IT|      null|  3|   Bob| 80000|\n|  28|   Finance|1994-11-22|  4| Emily| 65000|\n|  41|        HR|1981-12-18|  5| David| 90000|\n|  33|   Finance|1989-07-05|  6| Susan| 75000|\n|  46|        IT|1976-03-15|  7|  Mike| 95000|\n|  30|   Finance|1992-06-30| 10|Sophie| 62000|\n|  25|   Finance|1997-02-28|  2| Alice| 90000|\n|  28|   Finance|1994-11-22|  4| Emily| 70000|\n|  39|        IT|1983-10-14|  9| James| 87000|\n|  30|        IT|1992-05-12|  1|  John| 70000|\n|  27|        HR|1995-08-20|  8|  Lisa| 58000|\n+----+----------+----------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.format(\"json\").load(\"dbfs:/FileStore/tables/json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d27cbb9-5ca5-4fcf-a705-fd538d79ddcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- department: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7372b400-ec35-4950-8fd4-28cff8c75e64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  9| James|1983-10-14|  39| 87000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "new_json =  df_json.select(df_csv.columns) \n",
    "new_json.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60c5d46-1d0d-442f-92d2-f826a73a5115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  9| James|1983-10-14|  39| 87000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = df_csv.union(new_json)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1aa1b7-5198-426b-bf5b-161b2f7327e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'dob', 'age', 'salary', 'department'] ['id', 'name', 'dob', 'age', 'salary', 'department']\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.columns,new_json.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f6cf5b-3532-40a2-a796-68d71ab57df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d975bf-5ef7-46ab-bff4-58dc5e2888fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+-----+\n| id|  name|       dob| age|salary|department|count|\n+---+------+----------+----+------+----------+-----+\n|  4| Emily|1994-11-22|  28| 65000|   Finance|    2|\n|  2| Alice|1997-02-28|  25| 60000|        HR|    2|\n|  1|  John|1992-05-12|  30| 70000|        IT|    3|\n|  3|   Bob|      null|null| 80000|        IT|    2|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|    1|\n|  5| David|1981-12-18|  41| 90000|        HR|    1|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|    1|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|    1|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|    1|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|    1|\n|  9| James|1983-10-14|  39| 87000|        IT|    1|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|    1|\n+---+------+----------+----+------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df.groupBy(df.columns).count()\n",
    "duplicate_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0625b2-66c4-4b93-a479-b11825a4a333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97eb394-4f32-461e-8b9b-00cfba0da539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(df.columns).orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515edcf4-c2c8-4ba6-b97b-11c8a15f07f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+\n| id|  name|       dob| age|salary|department|is_duplicate|\n+---+------+----------+----+------+----------+------------+\n|  1|  John|1992-05-12|  30| 70000|        IT|       false|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       false|\n|  2| Alice|1997-02-28|  25| 60000|        HR|        true|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|       false|\n|  3|   Bob|      null|null| 80000|        IT|       false|\n|  3|   Bob|      null|null| 80000|        IT|        true|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       false|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|        true|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|       false|\n|  5| David|1981-12-18|  41| 90000|        HR|       false|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|       false|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|       false|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|       false|\n|  9| James|1983-10-14|  39| 87000|        IT|       false|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|       false|\n+---+------+----------+----+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\n",
    "    \"is_duplicate\",\n",
    "    F.row_number().over(w) > 1   # First value = false , Duplicates = True\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a912d8-1ebc-4977-be71-eb666d10a857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = duplicate_count.withColumn(\n",
    "    \"is_duplicate\",\n",
    "    F.when( F.col(\"count\") >1,True).otherwise(False)) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4046686316589654,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark task 1 (Find Duplicates)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
