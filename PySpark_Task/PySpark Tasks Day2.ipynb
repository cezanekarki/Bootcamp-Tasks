{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96626608-1240-4561-9f0c-97b185ce8095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 1\n",
    "load both csv and json union them properly.\n",
    "\n",
    "Instead of dropping duplicates, create a boolean column is_duplicate and set False to only one row and True to rest of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d98f74-c015-49fd-941f-ba49e61da5f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField,IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556c579b-8f16-4711-892e-fcb2ba931136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- age: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/FileStore/tables/csv/batch.csv\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bc27ff-a38b-4d77-8e54-e7b4366224dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sch = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"dob\", DateType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"salary\", IntegerType()),\n",
    "    StructField(\"department\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c47371a-887b-41b4-a6b4-e740b8d1d2c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.format(\"csv\").schema(sch).option(\"header\", True).load(\"dbfs:/FileStore/tables/csv/batch.csv\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a74296-3f4c-4dd8-ae0c-bb2fc996eb48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----+------+----------+\n| id| name|       dob| age|salary|department|\n+---+-----+----------+----+------+----------+\n|  1| John|1992-05-12|  30| 70000|        IT|\n|  2|Alice|1997-02-28|  25| 60000|        HR|\n|  3|  Bob|      null|null| 80000|        IT|\n|  4|Emily|1994-11-22|  28| 65000|   Finance|\n+---+-----+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1854e929-1180-4cd5-9562-016f6c809fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- department: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.format(\"json\").schema(sch).load(\"dbfs:/FileStore/tables/json\").orderBy(\"id\")\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5929b542-ca4f-4b2f-8128-d86088f6dd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n|  9| James|1983-10-14|  39| 87000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61fce8b-c83a-41ba-83a4-2079c3d3fe57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: '\\ndf_json = df_json.select(df_csv.columns)\\ndf_json.show()\\n'"
     ]
    }
   ],
   "source": [
    "# to change the order of columns \n",
    "# not needed if schema passed manually\n",
    "'''\n",
    "df_json = df_json.select(df_csv.columns)\n",
    "df_json.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5513d3ab-7dab-4e08-8851-a3441f486e3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+\n| id|  name|       dob| age|salary|department|\n+---+------+----------+----+------+----------+\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  1|  John|1992-05-12|  30| 70000|        IT|\n|  2| Alice|1997-02-28|  25| 60000|        HR|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|\n|  3|   Bob|      null|null| 80000|        IT|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|\n|  5| David|1981-12-18|  41| 90000|        HR|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|\n|  9| James|1983-10-14|  39| 87000|        IT|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|\n+---+------+----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = df_csv.union(df_json)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879ae287-75bc-437d-9e28-92aa75bac01f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bf10e7-0e9c-4142-9af7-7ac2ba32af36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create window object for duplicates\n",
    "w = Window.partitionBy(df.columns).orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2382c810-6835-49f7-9d49-0f177ae27a19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+----+------+----------+------------+\n| id|  name|       dob| age|salary|department|is_duplicate|\n+---+------+----------+----+------+----------+------------+\n|  1|  John|1992-05-12|  30| 70000|        IT|       false|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  1|  John|1992-05-12|  30| 70000|        IT|        true|\n|  2| Alice|1997-02-28|  25| 60000|        HR|       false|\n|  2| Alice|1997-02-28|  25| 60000|        HR|        true|\n|  2| Alice|1997-02-28|  25| 90000|   Finance|       false|\n|  3|   Bob|      null|null| 80000|        IT|       false|\n|  3|   Bob|      null|null| 80000|        IT|        true|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|       false|\n|  4| Emily|1994-11-22|  28| 65000|   Finance|        true|\n|  4| Emily|1994-11-22|  28| 70000|   Finance|       false|\n|  5| David|1981-12-18|  41| 90000|        HR|       false|\n|  6| Susan|1989-07-05|  33| 75000|   Finance|       false|\n|  7|  Mike|1976-03-15|  46| 95000|        IT|       false|\n|  8|  Lisa|1995-08-20|  27| 58000|        HR|       false|\n|  9| James|1983-10-14|  39| 87000|        IT|       false|\n| 10|Sophie|1992-06-30|  30| 62000|   Finance|       false|\n+---+------+----------+----+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = df.withColumn(\n",
    "    \"is_duplicate\", \n",
    "    F.row_number().over(w) > 1   # First value = false , Duplicates = True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "effa6b16-3d68-4c54-96d6-f7fe7f93cfee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task 2\n",
    "Calculate mean salary and check if it is greater or equal to the salary of employees in each department. -1\n",
    "\n",
    "Calculate mean salary and check if it is greater or equal to the salary of all employees. boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fee8952-5118-46fe-a5b6-88d75f43fc13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Tasks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
